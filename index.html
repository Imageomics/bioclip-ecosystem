<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BioCLIP Ecosystem | Foundation Models for the Tree of Life</title>
    <meta name="description" content="Central hub for the BioCLIP project. Find the right biological vision model for your research.">
    
    <link rel="stylesheet" href="css/variables.css">
    <link rel="stylesheet" href="css/style.css">
    <link rel="icon" type="image/png" href="https://github.com/Imageomics/Imageomics-guide/raw/3478acc0068a87a5604069d04a29bdb0795c2045/docs/logos/Imageomics_logo_butterfly.png">
    <script src="js/load-components.js" defer></script>
</head>
<body>

    <section class="hero">
        <div class="container">
            <div class="hero-inner">
                <!-- Left: Text content -->
                <div class="hero-text">
                    <h1>Vision Foundation Models<br>for the Tree of Life</h1>
                    <p class="home-hero hero-description">
                        Bridging computer vision and biology. BioCLIP models learn hierarchical representations
                        of the natural world, enabling advanced species classification, trait prediction, and more.
                    </p>
                    <div class="hero-actions">
                        <a href="#guide" class="btn btn-primary">Choose Your Tool</a>
                        <a href="https://huggingface.co/collections/imageomics/bioclip" target="_blank" rel="noopener noreferrer" class="btn btn-outline">Explore Collection</a>
                    </div>
                </div>

                <!-- Right: Video -->
                <div class="hero-visual">
                    <div class="hero-video-wrap">
                        <video class="hero-video" autoplay muted loop playsinline disablepictureinpicture controlsList="nodownload nofullscreen noremoteplayback" x-webkit-airplay="deny">
                            <source src="images/classifying-taxa.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section id="guide" class="section">
        <div class="container">
            <h2 class="section-title">Which BioCLIP component do you need?</h2>
            <!-- Does BioCAP offer language returns or is it still just classification? -->
            
            <div class="decision-guide">
                <div class="guide-card">
                    <span class="guide-icon">üöÄ</span>
                    <h3 class="guide-title">State-of-the-Art</h3>
                    <p class="guide-desc">I need the latest model with the highest accuracy (ViT-L) and emergent biological capabilities.</p>
                    <a href="#bioclip-2" class="guide-link">Go to BioCLIP 2 &rarr;</a>
                </div>

                <div class="guide-card">
                    <span class="guide-icon">üêç</span>
                    <h3 class="guide-title">Easy Python Integration</h3>
                    <p class="guide-desc">I want to use BioCLIP in my code without dealing with complex ML infrastructure.</p>
                    <a href="#pybioclip" class="guide-link">Go to pybioclip &rarr;</a>
                </div>

                <div class="guide-card">
                    <span class="guide-icon">üìú</span>
                    <h3 class="guide-title">Original</h3>
                    <p class="guide-desc">I need the original BioCLIP ViT-B model described in the 2024 CVPR paper.</p>
                    <a href="#bioclip" class="guide-link">Go to BioCLIP &rarr;</a>
                </div>

                <div class="guide-card">
                    <span class="guide-icon">üíæ</span>
                    <h3 class="guide-title">Raw Data & Models</h3>
                    <p class="guide-desc">I want to download weights, datasets (TreeOfLife-10M/200M), benchmarks, or try demos.</p>
                    <a href="#collection" class="guide-link">Go to HF Collection &rarr;</a>
                </div>
            </div>
        </div>
    </section>

    <section id="bioclip-2" class="section">
        <div class="container">
            <div class="component-row">
                <div class="component-text">
                    <span class="tag new">Latest Release</span>
                    <h2>BioCLIP 2</h2>
                    <p>
                        The next generation model, BioCLIP 2 was trained on <strong>214-million images</strong> across more than <strong>950-thousand taxa</strong>.
                        It outperforms BioCLIP by 18.0% and provides a 30.1% improvement over the CLIP (ViT-L/14) model used as weight initialization.
                    </p>
                    <p>
                        Beyond simple classification, it exhibits emergent properties, such as distinguishing between life stages, sexes, and aligning embeddings with ecological traits like beak size.
                    </p>
                    <ul class="feature-list">
                        <li><strong>Architecture:</strong> ViT-L/14 (Large)</li>
                        <li><strong>Training Data:</strong> 214 Million images (TreeOfLife-200M)</li>
                        <li><strong>Best for:</strong> High-accuracy tasks, fine-grained trait analysis, zero-shot learning.</li>
                    </ul>
                    <a href="https://imageomics.github.io/bioclip-2/" class="btn btn-primary">Visit BioCLIP 2 Site</a>
                    <a href="https://huggingface.co/imageomics/bioclip-2" class="btn btn-outline ml-2">Model Card</a>
                </div>
                <div class="component-image img-bioclip2">
                    <img src="images/bioclip2-ogimage.png" alt="BioCLIP 2 model visualization showing the model architecture, a clustered embedding plot with organism thumbnails, showing the separation by age and sex orthogonal to the species axis" loading="lazy" style="max-width:100%;height:auto;display:block;object-fit:contain;">
                </div>
            </div>
        </div>
    </section>

    <section id="pybioclip" class="section bg-light">
        <div class="container">
            <div class="component-row">
                <div class="component-text">
                    <span class="tag tool">Developer Tool</span>
                    <h2>pybioclip</h2>
                    <p>
                        A user-friendly Python package and command-line tool designed to make BioCLIP models accessible to everyone, regardless of ML expertise.
                        pybioclip provides easy-to-use interfaces for taxonomic classification, custom label prediction, and image embedding generation through your choice of BioCLIP model.
                        This can be accomplished through either a command-line interface (CLI) or a Python API for custom scripts.
                    </p>
                    <p>
                        Key features include:
                        <ul class="feature-list">
                            <li><b>Taxonomic label prediction</b> for images across ranks in the Linnaean hierarchy (tunable from kingdom to species).</li>
                            <li><b>Custom label predictions</b> from user-supplied classification categories.</li>
                            <li><b>Image embedding generation</b> in a text-aligned feature space.</li>
                            <li><b>Batch image processing</b> with performance optimizations.</li>
                            <li><b>Containers</b> provided to simplify use in computational pipelines.</li>
                        </ul>
                    </p>
                    <a href="https://imageomics.github.io/pybioclip/" class="btn btn-primary">Read Documentation</a>
                    <a href="https://github.com/Imageomics/pybioclip" class="btn btn-outline ml-2">View on GitHub</a>
                </div>
                <div class="component-image img-pybioclip">
                    pybioclip CLI
                </div>
            </div>
        </div>
    </section>

    <section id="bioclip" class="section">
        <div class="container">
            <div class="component-row">
                <div class="component-text">
                    <span class="tag">Original Model</span>
                    <h2>BioCLIP (Original)</h2>
                    <p>
                        The original foundation model presented in "BioCLIP: A Vision Foundation Model for the Tree of Life". It established the standard for using CLIP architectures in organismal biology.
                    </p>
                    <p>
                        Trained on the TreeOfLife-10M dataset, it covers over 450,000 taxa and learns a hierarchical representation that aligns with the biological taxonomy.
                    </p>
                    <ul class="feature-list">
                        <li><strong>Architecture:</strong> ViT-B/16 (Base)</li>
                        <li><strong>Training Data:</strong> 10 Million images</li>
                        <li><strong>Best for:</strong> General purpose baselines, reproducing original paper results.</li>
                    </ul>
                    <a href="https://imageomics.github.io/bioclip/" class="btn btn-primary">Visit BioCLIP Site</a>
                    <a href="https://arxiv.org/abs/2311.18803" class="btn btn-outline ml-2">Read Paper</a>
                </div>
                <div class="component-image img-bioclip">
                    <img src="images/bioclip-hook.svg" alt="BioCLIP model visualization showing the model architecture" loading="lazy" style="max-width:100%;height:auto;display:block;object-fit:contain;">
                </div>
            </div>
        </div>
    </section>

    <section id="collection" class="section bg-light">
        <div class="container">
            <div class="component-row">
                <div class="component-text">
                    <span class="tag tool">Repository</span>
                    <h2>Hugging Face Collection</h2>
                    <p>
                        The central warehouse for all BioCLIP assets. This collection aggregates all versions of the models, the training datasets, benchmarks, and interactive demos.
                    </p>
                    <p>
                        Use this if you need direct access to raw model weights (SafeTensors/PyTorch), want to access the TreeOfLife or benchmark datasets, or are looking for easy by-image predictions.
                    </p>
                    <ul class="feature-list">
                        <li><strong>Models:</strong> BioCLIP, BioCLIP 2, BioCAP.</li>
                        <li><strong>Datasets:</strong> TreeOfLife-200M, TreeOfLife-10M, TreeOfLife-10M Captions, Rare Species, IDLE-OO Camera Traps.</li>
                        <li><strong>Demos:</strong> Interactive Gradio apps for zero-shot and open-ended classification.</li>
                    </ul>
                    <a href="https://huggingface.co/collections/imageomics/bioclip" class="btn btn-primary">Browse Collection</a>
                </div>
                <div class="component-image img-hf">
                    ü§ó Hugging Face Collection
                </div>
            </div>
        </div>
    </section>
</body>
</html>
