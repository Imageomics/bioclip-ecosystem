<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Papers | BioCLIP Ecosystem</title>
    <meta name="description" content="Research papers and publications from the BioCLIP project, including BioCLIP, BioCLIP 2, and BioCAP.">
    
    <link rel="stylesheet" href="../css/variables.css">
    <link rel="stylesheet" href="../css/style.css">
    <link rel="icon" type="image/png" href="https://github.com/Imageomics/Imageomics-guide/raw/3478acc0068a87a5604069d04a29bdb0795c2045/docs/logos/Imageomics_logo_butterfly.png">
    <script src="../js/load-components.js" defer></script>
</head>
<body>

    <section class="hero">
        <div class="container">
            <h1>Research Papers</h1>
            <p>
                Publications and preprints from the BioCLIP project. 
                Explore the foundational research behind our vision models for the Tree of Life.
            </p>
        </div>
    </section>

    <section class="section">
        <div class="container">
            <div class="papers-grid">
                <!-- BioCLIP 2 Paper -->
                <article class="paper-card">
                    <div class="paper-thumbnail paper-thumbnail-bioclip2">
                        <img src="../images/bioclip2-ogimage.png" alt="BioCLIP 2 architecture and embedding visualization" loading="lazy">
                    </div>
                    <div class="paper-content">
                        <span class="paper-venue preprint">NeurIPS 2025</span>

                        <h2 class="paper-title">BioCLIP 2: Leveraging Generative Captions for Enhanced Biological Vision-Language Models</h2>
                        <p class="paper-authors">
                            Samuel Stevens, Jiaman Wu, Tharun Sripathi, Matthew J. Thompson, Loren Abraham, Elizabeth G. Campolongo, Joey Tianye Yu, Neha U. Hulkund, Sai Nammi, Anush Saipriyanka Ondugula, Wei-Lun Chao, Yu Su, Tanya Berger-Wolf, Charles V. Stewart
                        </p>
                        <p class="paper-abstract">
                            BioCLIP 2 is trained on 214 million images across 950,000+ taxa. It outperforms BioCLIP by 18.0% and exhibits emergent properties like distinguishing life stages and sexes.
                        </p>
                        <div class="paper-links">
                            <a href="https://doi.org/10.48550/arXiv.2505.23883" class="paper-link paper-link-arxiv" target="_blank" rel="noopener">arXiv</a>
                            <a href="https://imageomics.github.io/bioclip-2/" class="paper-link paper-link-site" target="_blank" rel="noopener">Project Site</a>
                            <a href="https://github.com/Imageomics/bioclip-2" class="paper-link paper-link-code" target="_blank" rel="noopener">Code</a>
                        </div>
                    </div>
                </article>

                <!-- BioCAP Paper -->
                <article class="paper-card">
                    <div class="paper-thumbnail paper-thumbnail-biocap">
                        <img src="../images/biocap-ogimage.png" alt="BioCAP caption generation visualization" loading="lazy">
                    </div>
                    <div class="paper-content">
                        <span class="paper-venue preprint">ICLR 2026</span>

                        <h2 class="paper-title">BioCAP: Generating Captions to Advance Shared Representations in Biology</h2>
                        <p class="paper-authors">
                            Tharun Sripathi, Jiaman Wu, Samuel Stevens, Matthew J. Thompson, Loren Abraham, Joey Tianyi Yu, Sai Nammi, Anush Saipriyanka Ondugula, Neha U. Hulkund, Elizabeth G. Campolongo, Tanya Berger-Wolf, Wei-Lun Chao, Yu Su, Charles V. Stewart
                        </p>
                        <p class="paper-abstract">
                            BioCAP enhances biological vision-language alignment by training with both taxonomic labels and generated captions, improving text-image understanding for organisms.
                        </p>
                        <div class="paper-links">
                            <a href="https://arxiv.org/abs/2510.20095" class="paper-link paper-link-arxiv" target="_blank" rel="noopener">arXiv</a>
                            <a href="https://imageomics.github.io/biocap/" class="paper-link paper-link-site" target="_blank" rel="noopener">Project Site</a>
                            <a href="https://huggingface.co/imageomics/biocap" class="paper-link paper-link-code" target="_blank" rel="noopener">Model</a>
                        </div>
                    </div>
                </article>

                <!-- BioCLIP Original Paper -->
                <article class="paper-card">
                    <div class="paper-thumbnail paper-thumbnail-bioclip">
                        <img src="../images/bioclip-hook.svg" alt="BioCLIP model architecture" loading="lazy">
                    </div>
                    <div class="paper-content">
                        <span class="paper-venue cvpr">CVPR 2024</span>
                        <h2 class="paper-title">BioCLIP: A Vision Foundation Model for the Tree of Life</h2>
                        <p class="paper-authors">
                            Samuel Stevens, Jiaman Wu, Matthew J. Thompson, Elizabeth G. Campolongo, Chan Hee Song, David Edward Carlyn, Li Dong, Wasila M. Dahdul, Charles V. Stewart, Tanya Berger-Wolf, Wei-Lun Chao, Yu Su
                        </p>
                        <p class="paper-abstract">
                            The foundational BioCLIP model trained on TreeOfLife-10M with 450,000+ taxa. Establishes the standard for using CLIP architectures in organismal biology.
                        </p>
                        <div class="paper-links">
                            <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Stevens_BioCLIP_A_Vision_Foundation_Model_for_the_Tree_of_Life_CVPR_2024_paper.html" class="paper-link" target="_blank" rel="noopener">Paper</a>
                            <a href="https://imageomics.github.io/bioclip/" class="paper-link paper-link-site" target="_blank" rel="noopener">Project Site</a>
                            <a href="https://github.com/Imageomics/bioclip" class="paper-link paper-link-code" target="_blank" rel="noopener">Code</a>
                        </div>
                    </div>
                </article>
            </div>
        </div>
    </section>

</body>
</html>
