<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BioCLIP Ecosystem | Foundation Models for the Tree of Life</title>
    <meta name="description" content="Central hub for the BioCLIP project. Find the right biological vision model for your research.">
    
    <link rel="stylesheet" href="../css/variables.css">
    <link rel="stylesheet" href="../css/style.css">
    <link rel="icon" type="image/png" href="https://github.com/Imageomics/Imageomics-guide/raw/3478acc0068a87a5604069d04a29bdb0795c2045/docs/logos/Imageomics_logo_butterfly.png">
    <script src="../js/load-components.js" defer></script>
</head>
<body>

    <section class="hero">
        <div class="container">
            <h1>Vision Foundation Models<br>for the Tree of Life</h1>
            <p>
                Bridging computer vision and biology. BioCLIP models learn hierarchical representations 
                of the natural world, enabling advanced species classification, trait prediction, and more.
            </p>
            <div class="hero-actions">
                <a href="#guide" class="btn btn-primary">Choose Your Model</a>
                <a href="https://huggingface.co/collections/imageomics/bioclip" target="_blank" class="btn btn-outline">Explore Collection</a>
            </div>
        </div>
    </section>

    <section id="guide" class="section">
        <div class="container">
            <h2 class="section-title">Which BioCLIP model do you need?</h2>
            <!-- BioCAP is it still just classification -->
            
            <div class="decision-guide">
                <div class="guide-card">
                    <span class="guide-icon">ðŸš€</span>
                    <h3 class="guide-title">State-of-the-Art</h3>
                    <p class="guide-desc">I need the latest model with the highest accuracy (ViT-L) and emergent biological capabilities.</p>
                    <a href="#bioclip-2" class="guide-link">Go to BioCLIP 2 &rarr;</a>
                </div>
                <div class="guide-card">
                    <span class="guide-icon">ðŸ“ƒ</span>
                    <h3 class="guide-title">Trained with Captions</h3>
                    <p class="guide-desc">I need a ViT-B/16 model for inference speed or direct comparison; looking for base model trained with captions and taxonomic labels.</p>
                    <!--want to try BioCAP, the original BioCLIP model, but trained with captions and taxonomic labels-->
                    <a href="#biocap" class="guide-link">Go to BioCAP &rarr;</a>
                </div>
                <div class="guide-card">
                    <span class="guide-icon">ðŸ“œ</span>
                    <h3 class="guide-title">Original</h3>
                    <p class="guide-desc">I need the original BioCLIP ViT-B model described in the 2024 CVPR paper.</p>
                    <a href="#bioclip" class="guide-link">Go to BioCLIP &rarr;</a>
                </div>
                <div class="guide-card">
                    <span class="guide-icon">ðŸ’¾</span>
                    <h3 class="guide-title">Raw Models</h3>
                    <p class="guide-desc">I want to download weights or try demos.</p>
                    <a href="#collection" class="guide-link">Go to HF Collection &rarr;</a>
                </div>
            </div>
        </div>
    </section>

    <section id="bioclip-2" class="section">
        <div class="container">
            <div class="component-row">
                <div class="component-text">
                    <span class="tag new">Latest Release</span>
                    <h2>BioCLIP 2</h2>
                    <p>
                        The next generation model, BioCLIP 2 was trained on <strong>214-million images</strong> across more than <strong>950-thousand taxa</strong>.
                        It outperforms BioCLIP by 18.0% and provides a 30.1% improvement over the CLIP (ViT-L/14) model used as weight initialization.
                    </p>
                    <p>
                        Beyond simple classification, it exhibits emergent properties, such as distinguishing between life stages, sexes, and aligning embeddings with ecological traits like beak size.
                    </p>
                    <ul class="feature-list">
                        <li><strong>Architecture:</strong> ViT-L/14 (Large)</li>
                        <li><strong>Training Data:</strong> 214 Million images (TreeOfLife-200M)</li>
                        <li><strong>Best for:</strong> High-accuracy tasks, fine-grained trait analysis, zero-shot learning.</li>
                    </ul>
                    <a href="https://imageomics.github.io/bioclip-2/" class="btn btn-primary">Visit BioCLIP 2 Site</a>
                    <a href="https://huggingface.co/imageomics/bioclip-2" class="btn btn-outline ml-2">Model Card</a>
                    <a href="https://doi.org/10.48550/arXiv.2505.23883" class="btn btn-outline ml-2">Read Paper</a>
                </div>
                <div class="component-image img-bioclip2">
                    <img src="../images/bioclip2-ogimage.png" alt="BioCLIP 2 model visualization showing the model architecture, a clustered embedding plot with organism thumbnails, showing the separation by age and sex orthogonal to the species axis" loading="lazy" style="max-width:100%;max-height:100%;display:block;object-fit:contain;">
                </div>
            </div>
        </div>
    </section>

    <section id="biocap" class="section bg-light">
        <div class="container">
            <div class="component-row">
                <div class="component-text">
                    <span class="tag tool">Trained with Captions</span>
                    <h2>BioCAP</h2>
                    <p>
                        BioCAP introduces descriptive captions to the original BioCLIP training regimen as complementary supervision, aligning visual and textual representations within the latent morphospace of species. This added context improves performance by <b>+8.8%</b> on classification and <b>+21.3%</b> on retrieval, demonstrating that descriptive language enriches biological foundation models beyond labels.
                    </p>
                    <p>
                        Trained on the TreeOfLife-10M dataset paired with TreeOfLife-10M Captions, providing synthetic, trait-rich captions generated based on morphological context and taxon-specific examples, it covers over 390,000 taxa and learns a hierarchical representation that aligns with the biological taxonomy.
                    </p>
                    <ul class="feature-list">
                        <li><strong>Architecture:</strong> ViT-B/16 (Base)</li>
                        <li><strong>Training Data:</strong> 10 Million images (TreeOfLife-10M) with associated taxonomic and morphological context-based synthetic captions (TreeOfLife-10M Captions)</li>
                        <li><strong>Best for:</strong> General purpose baselines (ViT-B/16 needed, for inference speed or direct comparison), caption generation pipelines, reproducing paper results.
                        </li>
                    </ul>
                    <a href="https://imageomics.github.io/biocap/" class="btn btn-primary">Visit BioCAP Site</a>
                    <a href="https://huggingface.co/imageomics/biocap" class="btn btn-outline ml-2">Model Card</a>
                    <a href="https://arxiv.org/abs/2510.20095" class="btn btn-outline ml-2">Read Paper</a>
                </div>
                <div class="component-image img-biocap">
                    <img src="../images/biocap-ogimage.png" alt="BioCAP model visualization showing the caption generation process, a clustered embedding plot with organism thumbnails, showing the separation by sex orthogonal to the 'fly' axis" loading="lazy" style="max-width:100%;max-height:100%;display:block;object-fit:contain;">
                </div>
            </div>
        </div>
    </section>

    <section id="bioclip" class="section">
        <div class="container">
            <div class="component-row">
                <div class="component-text">
                    <span class="tag">Original Model</span>
                    <h2>BioCLIP (Original)</h2>
                    <p>
                        The original foundation model presented in "BioCLIP: A Vision Foundation Model for the Tree of Life". It established the standard for using CLIP architectures in organismal biology.
                    </p>
                    <p>
                        Trained on the TreeOfLife-10M dataset, it covers over 450,000 taxa and learns a hierarchical representation that aligns with the biological taxonomy.
                    </p>
                    <ul class="feature-list">
                        <li><strong>Architecture:</strong> ViT-B/16 (Base)</li>
                        <li><strong>Training Data:</strong> 10 Million images (TreeOfLife-10M)</li>
                        <li><strong>Best for:</strong> Reproducing original paper results.</li>
                    </ul>
                    <a href="https://imageomics.github.io/bioclip/" class="btn btn-primary">Visit BioCLIP Site</a>
                    <a href="https://huggingface.co/imageomics/bioclip" class="btn btn-outline ml-2">Model Card</a>
                    <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Stevens_BioCLIP_A_Vision_Foundation_Model_for_the_Tree_of_Life_CVPR_2024_paper.html" class="btn btn-outline ml-2">Read Paper</a>
                </div>
                <div class="component-image img-bioclip">
                    <img src="../images/bioclip-hook.svg" alt="BioCLIP model visualization showing the model architecture" loading="lazy" style="max-width:100%;max-height:100%;display:block;object-fit:contain;">
                </div>
            </div>
        </div>
    </section>

    <section id="collection" class="section bg-light">
        <div class="container">
            <div class="component-row">
                <div class="component-text">
                    <span class="tag tool">Repository</span>
                    <h2>Hugging Face Collection</h2>
                    <p>
                        The central warehouse for all BioCLIP assets. This collection aggregates all versions of the models, the training datasets, benchmarks, and interactive demos.
                    </p>
                    <p>
                        Use this if you need direct access to raw model weights (SafeTensors/PyTorch), want to access the TreeOfLife or benchmark datasets, or are looking for easy by-image predictions.
                    </p>
                    <ul class="feature-list">
                        <li><strong>Models:</strong> BioCLIP, BioCLIP 2, BioCAP.</li>
                        <li><strong>Datasets:</strong> TreeOfLife-200M, TreeOfLife-10M, TreeOfLife-10M Captions.</li>
                        <li><strong>Benchmarks:</strong> Rare Species, IDLE-OO Camera Traps.</li>
                        <li><strong>Demos:</strong> Interactive Gradio apps for zero-shot and open-ended classification.</li>
                    </ul>
                    <a href="https://huggingface.co/collections/imageomics/bioclip" class="btn btn-primary">Browse Collection</a>
                </div>
                <div class="component-image img-hf">
                    ðŸ¤— Hugging Face Collection
                </div>
            </div>
        </div>
    </section>
</body>
</html>
